{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 2: POTUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) President of the United States (Trump vs. Obama)\n",
    "\n",
    "Surely, you're aware that the 45th President of the United States (@POTUS45) was an active user of Twitter, until (permanently) banned on Jan 8, 2021.\n",
    "You can still enjoy his greatness at the [Trump Twitter Archive](https://www.thetrumparchive.com/). We will be using original tweets only, so make sure to remove all retweets.\n",
    "Another fan of Twitter was Barack Obama (@POTUS43 and @POTUS44), who used the platform in a rather professional way.\n",
    "Please also consider the POTUS Tweets of Joe Biden; we will be using those for testing.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are multiple ways to get the data, but the easiest way is to download the files from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group. \n",
    "Another way is to directly use the data from [Trump Twitter Archive](https://www.thetrumparchive.com/), [Obama Kaggle](https://www.kaggle.com/jayrav13/obama-white-house), and [Biden Kaggle](https://www.kaggle.com/rohanrao/joe-biden-tweets).\n",
    "Before you get started, please download the files; you can put them into the data folder.\n",
    "\n",
    "### N-gram Models\n",
    "\n",
    "In this assignment, you will be doing some Twitter-related preprocessing and training n-gram models to be able to distinguish between Tweets of Trump, Obama, and Biden.\n",
    "We will be using [NLTK](https://www.nltk.org), more specifically it's [`lm`](https://www.nltk.org/api/nltk.lm.html) module. \n",
    "Install the NLTK package within your working environment.\n",
    "You can use some of the NLTK functions, but you have to implement the functions for likelihoods and perplexity from scratch.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import nltk\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Prepare all the Tweets. Since the `lm` modules will work on tokenized data, implement a tokenization method that strips unnecessary tokens but retains special words such as mentions (@...) and hashtags (#...).\n",
    "\n",
    "1.2 Partition into training and test sets; select about 100 tweets each, which we will be testing on later. As with any Machine Learning task, training and test must not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ae5b5d-fccd-4092-af20-d8e8b4a65ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: ignore retweets \n",
    "\n",
    "def load_tweets_from_csv(filepath, column_name):\n",
    "    tweets = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        for row in reader:\n",
    "            if column_name in row: \n",
    "                tweets.append(row[column_name])\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def load_tweets_from_json(filepath, key_name):\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file) \n",
    "        for entry in data:\n",
    "            if entry.get('isRetweet') == 'f' and key_name in entry:\n",
    "                tweets.append(entry[key_name])\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def load_trump_tweets(filepath):\n",
    "    \"\"\"Loads all Trump tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return load_tweets_from_json(filepath, \"text\")\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_obama_tweets(filepath):\n",
    "    \"\"\"Loads all Obama tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return load_tweets_from_csv(filepath, \"Tweet-text\")\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def load_biden_tweets(filepath):\n",
    "    \"\"\"Loads all Biden tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return load_tweets_from_csv(filepath, \"tweet\")\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25e8c56-3837-440b-bebe-1916ebede6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\André\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\André\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Notice: think about start and end tokens\n",
    "\n",
    "NUM_TEST = 100\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenizes a single Tweet.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    filtered = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered = ['<s>'] + filtered + ['</s>']\n",
    "    return filtered\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def split_and_tokenize(data, num_test=NUM_TEST):\n",
    "    \"\"\"Splits and tokenizes the given list of Twitter tweets.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    split_index = len(data) - num_test\n",
    "    \n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    tokenized_train = [tokenize(tweet) for tweet in train_data]\n",
    "    tokenized_test = [tokenize(tweet) for tweet in test_data]\n",
    "    \n",
    "    return tokenized_train, tokenized_test\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2598cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets = split_and_tokenize(load_trump_tweets(\"./data/tweets_01-08-2021.json\"))\n",
    "obama_tweets = split_and_tokenize(load_obama_tweets(\"./data/Tweets-BarackObama.csv\"))\n",
    "biden_tweets = split_and_tokenize(load_biden_tweets(\"./data/JoeBidenTweets.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666d979c-0daa-4934-b68f-8b09f4398d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'ultimate', 'goal', 'agreement', 'gets', 'deficit', 'control', 'way', 'fair', 'balanced', 'President', 'Obama', '</s>'], ['<s>', 'voices', 'American', 'people', 'part', 'debate', 'President', 'Obama', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "print(obama_tweets[1][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32c33-1a19-42ac-93d0-fcf66fbeaf5f",
   "metadata": {},
   "source": [
    "### Train N-gram Models\n",
    "\n",
    "2.1 Train n-gram models with n = [1, ..., 5] for Obama, Trump, and Biden.\n",
    "\n",
    "2.2 Also train a joint model, that will serve as background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0828bda0-cef2-428b-a238-7f07f2c25425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_gram_models(n, data):\n",
    "    \"\"\"\n",
    "    To predict the first few words of the Tweet, we need the smaller n-grams as\n",
    "    well. This method does calculate all n-grams up to the given n.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    models = {}\n",
    "    for i in range(1, n+1):\n",
    "        n_gram_counts = {}\n",
    "        prefix_counts = {}\n",
    "        for tweet in data:\n",
    "            # Generate n-grams and their respective prefixes\n",
    "            # Ensure everything is a tuple to avoid TypeError\n",
    "            n_grams = [tuple(tweet[j:j+i]) for j in range(len(tweet) - i + 1)]\n",
    "            prefixes = [tuple(tweet[j:j+i-1]) for j in range(len(tweet) - i + 1)] if i > 1 else [('<s>',) * (i-1)] * (len(tweet) - i + 1)\n",
    "\n",
    "            for n_gram in n_grams:\n",
    "                if n_gram in n_gram_counts:\n",
    "                    n_gram_counts[n_gram] += 1\n",
    "                else:\n",
    "                    n_gram_counts[n_gram] = 1\n",
    "\n",
    "            for prefix in prefixes:\n",
    "                if prefix in prefix_counts:\n",
    "                    prefix_counts[prefix] += 1\n",
    "                else:\n",
    "                    prefix_counts[prefix] = 1\n",
    "\n",
    "        model = {}\n",
    "        for n_gram, count in n_gram_counts.items():\n",
    "            prefix = n_gram[:-1]\n",
    "            model[n_gram] = count / prefix_counts[prefix]\n",
    "        models[i] = model\n",
    "\n",
    "    return models\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_suggestion(prev, n_gram_model):\n",
    "    \"\"\"\n",
    "    Gets the next random word for the given n_grams.\n",
    "    The size of the previous tokens must be exactly one less than the n-value\n",
    "    of the n-gram, or it will not be able to make a prediction.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    if len(prev) != len(next(iter(n_gram_model.keys()))) - 1:\n",
    "        raise ValueError(\"The size of the previous tokens must match the n-1 value of the n-gram model.\")\n",
    "\n",
    "    possible_continuations = {key[-1]: n_gram_model[key] for key in n_gram_model if key[:-1] == prev}\n",
    "\n",
    "    if not possible_continuations:\n",
    "        return None\n",
    "\n",
    "    next_words = list(possible_continuations.keys())\n",
    "    probabilities = list(possible_continuations.values())\n",
    "\n",
    "    next_word = random.choices(next_words, weights=probabilities, k=1)[0]\n",
    "\n",
    "    return next_word\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_random_tweet(n, n_gram_models):\n",
    "    \"\"\"Generates a random tweet using the given data set.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    if n not in n_gram_models:\n",
    "        raise ValueError(\"Specified order n is not available in the provided n-gram models.\")\n",
    "\n",
    "    current_tweet = ['<s>']\n",
    "    sentence_finished = False\n",
    "\n",
    "    while not sentence_finished:\n",
    "        current_context = tuple(current_tweet[-(n-1):])\n",
    "        next_word = get_suggestion(current_context[:min(len(current_context) - 1, n-1)], n_gram_models[min(len(current_context), n)])\n",
    "        if next_word == '</s>' or next_word is None:\n",
    "            sentence_finished = True\n",
    "        else:\n",
    "            current_tweet.append(next_word)\n",
    "\n",
    "    current_tweet.append(\"</s>\")\n",
    "    final_tweet = ' '.join(current_tweet)\n",
    "    return final_tweet\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37e5cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> playing Dallas way violent big release American time resort Trump run </s>\n"
     ]
    }
   ],
   "source": [
    "n_gram_models = build_n_gram_models(2, trump_tweets[0])\n",
    "random_tweet_trump = get_random_tweet(2, n_gram_models)\n",
    "print(random_tweet_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648216bb-a49e-45ff-bb8c-9094c33acc07",
   "metadata": {},
   "source": [
    "### Classify the Tweets\n",
    "\n",
    "3.1 Use the log-ratio method to classify the Tweets for Trump vs. Biden. Trump should be easy to spot; but what about Obama vs. Biden?\n",
    "\n",
    "3.2 Analyze: At what context length (n) does the system perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99dd4ca5-8094-40b0-aa1b-a51268659397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_token_log_ratio(prev, token, n_gram_model1, n_gram_model2):\n",
    "    \"\"\"Calculates the log ration of a token for two different n-grams\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "\n",
    "    prob1 = n_gram_model1.get((prev + (token,)), epsilon)\n",
    "    prob2 = n_gram_model2.get((prev + (token,)), epsilon)\n",
    "    \n",
    "    log_ratio = math.log(prob1 + epsilon) - math.log(prob2 + epsilon)\n",
    "    \n",
    "    return log_ratio\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def classify(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    log_ratio_sum = 0\n",
    "    for i in range(n - 1, len(tokens) - 1):\n",
    "        prev = tuple(tokens[i - (n - 1):i])\n",
    "        token = tokens[i]\n",
    "        log_ratio = calculate_single_token_log_ratio(prev, token, n_gram_models1, n_gram_models2)\n",
    "        log_ratio_sum += log_ratio\n",
    "    \n",
    "    if log_ratio_sum > 0:\n",
    "        return True\n",
    "    elif log_ratio_sum <= 0:\n",
    "        return False\n",
    "    \n",
    "    ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d1e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(n, data1, data2, classify_fn):\n",
    "    \"\"\"\n",
    "    Trains the n-gram models on the train data and validates on the test data.\n",
    "    Uses the implemented classification function to predict the Tweeter.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    train_data_1, test_data_1 = data1[0], data1[1]\n",
    "    train_data_2, test_data_2 = data2[0], data2[1]\n",
    "\n",
    "    n_gram_model_1 = build_n_gram_models(n, train_data_1)[n]\n",
    "    n_gram_model_2 = build_n_gram_models(n, train_data_2)[n]\n",
    "\n",
    "    test_data = test_data_1 + test_data_2\n",
    "    test_labels = [True] * len(test_data_1) + [False] * len(test_data_2)  # True for 1, False for 2\n",
    "\n",
    "    # Validate on test data\n",
    "    correct_predictions = 0\n",
    "    for tweet, label in zip(test_data, test_labels):\n",
    "        prediction = classify_fn(n, tweet, n_gram_model_1, n_gram_model_2)\n",
    "        if prediction == label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5f88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "context_length = 2\n",
    "validate(context_length, trump_tweets, biden_tweets, classify_fn=classify)\n",
    "validate(context_length, obama_tweets, biden_tweets, classify_fn=classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e1fb7-c67b-4e44-87c7-488e704c5ac1",
   "metadata": {},
   "source": [
    "### Compute Perplexities\n",
    "\n",
    "4.1 Compute (and plot) the perplexities for each of the test tweets and models. Is picking the Model with minimum perplexity a better classifier than in 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfe2154-d816-442e-8de8-3b836ab0ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_of_sequence(tokens, n_gram_model, n):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a sequence given an n-gram model.\n",
    "    \"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return float('inf')\n",
    "    log_prob_of_sequence = 0\n",
    "    total_n_grams = len(tokens) - n + 1\n",
    "\n",
    "    for i in range(total_n_grams):\n",
    "        n_gram = tuple(tokens[i:i+n])\n",
    "        if n_gram in n_gram_model:\n",
    "            probability = n_gram_model[n_gram]\n",
    "        else:\n",
    "            probability = 1e-10 \n",
    "\n",
    "        log_prob_of_sequence += math.log(probability)\n",
    "\n",
    "    return math.pow(2, -log_prob_of_sequence / total_n_grams)\n",
    "\n",
    "\n",
    "def classify_with_perplexity(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    perplexity1 = calculate_perplexity_of_sequence(tokens, n_gram_models1, n)\n",
    "    perplexity2 = calculate_perplexity_of_sequence(tokens, n_gram_models2, n)\n",
    "    return perplexity1 < perplexity2 \n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b3be177",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "context_length = 2\n",
    "validate(context_length, trump_tweets, biden_tweets, classify_fn=classify_with_perplexity)\n",
    "validate(context_length, obama_tweets, biden_tweets, classify_fn=classify_with_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
