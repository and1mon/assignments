{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 3: Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Isolated Word Recognition\n",
    "\n",
    "In this assignment, we'll be revising word recognition, this time using Hidden Markov Models (HMM).\n",
    "As with [assignment 1](https://github.com/seqlrn/assignments/tree/master/1-dynamic-programming), we'll be using the [free spoken digits](https://github.com/Jakobovski/free-spoken-digit-dataset) dataset.\n",
    "We will be using the [`pandas`](https://pandas.pydata.org/docs/) library for data handling and [`hmmlearn`](https://hmmlearn.readthedocs.io/en/latest/index.html) library for HMMs which depends on `numpy`.\n",
    "Install the `pandas` and `hmmlearn` packages in your working environment and get familiar with these modules.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Download Zohar Jackson's [free spoken digit](https://github.com/Jakobovski/free-spoken-digit-dataset) dataset.\n",
    "There's no need to clone, feel free to use a revision, like [v1.0.10](https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz).\n",
    "The file naming convention is `{digitLabel}_{speakerName}_{index}.wav`.\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "As you can learn from the [tutorial](https://hmmlearn.readthedocs.io/en/latest/tutorial.html#), `hmmlearn` provides us with the base implementation of Hidden Markov Models; we'll be using the `hmm.GaussianHMM`, which implements HMMs with a single Gaussian emission probability per state.\n",
    "For a starter, build a basic isolated word recognizer that uses a separate model for each digit.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 To facilitate the selection of samples for speakers and digits, consider how you can store the data within a `pandas.DataFrame`.\n",
    "\n",
    "1.2 Compute the MFCC features for the complete data set (3000 recordings; use `n_mfcc=13`).\n",
    "\n",
    "1.3 Apply per-speaker feature normalization (e.g., standardization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee275c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 50 # recordings per speaker & digit\n",
    "DIGITS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "SPEAKERS = [\"george\", \"jackson\", \"lucas\", \"nicolas\", \"theo\", \"yweweler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: a good default value is 25ms for FFT window and 10ms for hop length\n",
    "### Notice: be careful as librosa takes the number of samples as input!        \n",
    "\n",
    "def compute_features(file):\n",
    "    \"\"\"Computes the features for a recording file.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_dataframe(input_dir):\n",
    "    \"\"\"Loads the recordings into a pandas.DataFrame.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def normalize_features(dataframe):\n",
    "    \"\"\"Applies per-speaker feature normalization.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7466a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"<path to your recordings>\"\n",
    "dataframe = load_dataframe(input_dir=INPUT_DIR)\n",
    "dataframe_w_norm = normalize_features(dataframe=dataframe)\n",
    "\n",
    "### Notice: just for test purposes\n",
    "\n",
    "# print(\"Num recordings: {}\".format(len(dataframe)))\n",
    "# for speaker in SPEAKERS:\n",
    "#     print(\"### {}\".format(speaker))\n",
    "#     data_speaker = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "\n",
    "#     print(data_speaker[\"digit\"].value_counts())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement a 6-fold [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) loop for the 6 speakers to (later) figure out, which test speaker performs best/worst. That is, each speaker acts as test speaker while the others are used for training (with each possible combination).\n",
    "\n",
    "2.2 Inside the cross-validation loop, train an individual HMM with linear topology for each digit. There are several points to consider:\n",
    "\n",
    "*The [`fit`](https://github.com/hmmlearn/hmmlearn/blob/38b3cece4a6297e978a204099ae6a0a99555ec01/lib/hmmlearn/base.py#L439) expects features to be sequential in a single array with `X` as (n_train_samples, n_features). Furthermore, we need to pass the lengths of each recording into the function with`lengths` as (n_samples,):*\n",
    "\n",
    "```python\n",
    "### you can flatten the features of the train data as follows\n",
    "\n",
    "# input: [(rec_samples_1, n_feats), ..., (rec_samples_N, n_feats)]\n",
    "# output: (all_rec_samples, n_feats)\n",
    "features = [features for features in dataframe[\"features\"].values]\n",
    "flatten = np.concatenate(features, axis=0)\n",
    "\n",
    "lengths = np.array([...])\n",
    "\n",
    "# train HMM\n",
    "hmm.fit(X=flatten, lengths=lengths)\n",
    "```\n",
    "\n",
    "*For the HMM, it is necessary to choose a meaningful number of states. How many states (`n_components`) do you choose, and why?*\n",
    "\n",
    "*With respect to the used `hmmlearn` library. How can you enforce a linear topology?*\n",
    "\n",
    "*You might find that certain digits perform particularly bad; what could be a reason and how to mitigate it?*\n",
    "    \n",
    "2.3 Compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for each speaker and for the overall dataset by combining the predictions of the cross-validation. You can use the [`scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) library.\n",
    "\n",
    "2.4 Additional experiment: Compare the results without and with per-speaker feature normalization. How does the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e903d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO:\n",
    "### 1. set the `n_components` for all digits (choose a meaningful number of states)\n",
    "\n",
    "n_comps = {i: None for i in DIGITS}\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "### 1. implement the 6-fold cross-validation loop\n",
    "### 2. allocate and initialize the HMMs, one for each digit; set a linear topology\n",
    "### 3. train the HMMs using the fit method; data needs to be concatenated\n",
    "### 4. evaluate the trained models on the test speaker; how do you decide which word\n",
    "###    was spoken?\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "### 1. based on the results, compute and display the confusion matrix for \n",
    "###    each test speaker \n",
    "### 2. compute and display the confusion matrix for the overall dataset\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278660a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) Decoding Sequences of Digits\n",
    "\n",
    "The example above can't handle sequences of spoken digits.\n",
    "In this part of the assignment, you'll build a basic decoder that is able to decode arbitrary sequences of digits (without a prior, though).\n",
    "The `decode` method in `hmmlearn` only works for a single HMM.\n",
    "There are two ways how to solve this assignment:\n",
    "\n",
    "- Construct a \"meta\" HMM from the previously trained digit HMMs, by allowing state transitions from one digit to another; the resulting HMM can be decoded using the existing `decode` method (don't forget to re-map the state ids to the originating digit).\n",
    "\n",
    "- (Optional) Implement a real (time-synchronous) decoder using beam search. The straight-forward way is to maintain a (sorted) list of active hypotheses (ie. state history and current log-likelihood) that is first expanded and then pruned in each time step. The tricky part is at the \"end\" of a model: do you loop or expand new words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Test Sequences\n",
    "\n",
    "3.1 Generate a few test sequences of random length in between 3 and 6 digits; use [`numpy.random.randint`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) and be sure to also retain the digits sequence since we need to compute edit distance between reference and hypotheses later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b562cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digit_sequence(speaker_dataframe, min_digits, max_digits):\n",
    "    \"\"\"\n",
    "    Creates a sequence of spoken digits from a speaker and returns the\n",
    "    features and reference label.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eda3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: just for test purposes\n",
    "\n",
    "# speaker = \"george\"\n",
    "# data_george = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "# for i in range(20):\n",
    "#     data_seq, digits = create_digit_sequence(data_george)\n",
    "#     print(\"Digits: {}\".format(digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f453d4",
   "metadata": {},
   "source": [
    "### Create \"meta\" HMM\n",
    "\n",
    "4.1 Combine the previously trained HMMs to a single \"meta\" HMM, altering the transition probabilities to make a circular graph that allows each word to follow another.\n",
    "\n",
    "4.2 Implement a method that converts a state sequence relating to the meta HMM into a sequence of actual digits.\n",
    "\n",
    "4.3 Decode your test sequences and compute the [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) with [JiWER](https://pypi.org/project/jiwer/) (install the package in your working environment).\n",
    "\n",
    "4.4 Compute an overall WER; ie. over the cross-validation.\n",
    "\n",
    "4.5 (Optional) Implement a basic time-synchronous beam search; how do the results compare to the above viterbi decoding in terms of accuracy and time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
