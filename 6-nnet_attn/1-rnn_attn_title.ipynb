{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 6: Attention (please!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Thesis Title Classification\n",
    "\n",
    "In this assignment, we'll again rely on the theses dataset and want to classify whether a thesis is bachelor or master.\n",
    "Update your B.Sc. / M.Sc. thesis title classification model from the previous assignment and integrate the attention mechanism.\n",
    "Therefore, implement the `dot product attention` and check how it affects the training and performance for this task.\n",
    "In case you want to start fresh, we provide some boiler plate code of a base RNN classification model as well as ready-to-go data loading.\n",
    "The basic setup as well as some code and steps can be reused from your solution for the RNN tasks.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sklearn_metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Filter out all diploma theses; they might be too easy to spot because they only cover \"old\" topics.\n",
    "\n",
    "1.4 Create a PyTorch Dataset class which handles your tokenized data with respect to input and (class) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theses_dataset(filepath):\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return pd.read_csv(filepath, header=0, sep=\";\")\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    def _preprocss_fn(text):\n",
    "        remove_digits = str.maketrans(string.digits, ' '*len(string.digits))\n",
    "        remove_pun = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(remove_digits)\n",
    "        text = text.translate(remove_pun)\n",
    "        text = re.sub(' {2,}', ' ', text)\n",
    "        return text.lower()\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Remove punctuation, digits and lowercase titles\n",
    "    dataframe[\"Titel\"] = dataframe[\"Titel\"].apply(lambda s: _preprocss_fn(s))\n",
    "\n",
    "    # Filter out empty and short titles\n",
    "    dataframe = dataframe[dataframe[\"Titel\"].str.len() > 4]\n",
    "\n",
    "    # Reset index of dataframe\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Simple tokenization of titles\n",
    "    dataframe[\"tokenized\"] = [title.split() for title in dataframe[\"Titel\"].values]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "dataframe_all = load_theses_dataset(\"data/theses2022.csv\")\n",
    "dataframe_all = dataframe_all[dataframe_all[\"Sprache\"] == \"DE\"]\n",
    "dataframe_all = preprocess(dataframe_all)\n",
    "\n",
    "# Convert labels to integer\n",
    "LABEL2IDX = {\"Bachelor\": 0, \"Master\": 1, \"Diplom\": 2}\n",
    "dataframe_all[\"label\"] = dataframe_all[\"Grad\"].apply(lambda l: LABEL2IDX[l])\n",
    "\n",
    "# Filter out `Diplom`\n",
    "dataframe_diplom = dataframe_all[dataframe_all[\"Grad\"] == \"Diplom\"]\n",
    "dataframe = dataframe_all[dataframe_all[\"Grad\"] != \"Diplom\"]\n",
    "\n",
    "# Check number of samples and label distribution\n",
    "print(f\"Num theses (overall): {len(dataframe_all)}\")\n",
    "print(f\"Num theses (w/o diplom): {len(dataframe)}\")\n",
    "print(f\"Num theses (diplom): {len(dataframe_diplom)}\")\n",
    "print()\n",
    "print(dataframe_all[\"Grad\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: Think about padding tokens for batch sizes > 1\n",
    "\n",
    "vocab = set()\n",
    "vocab.add(\"<pad>\")\n",
    "\n",
    "# For a more realistic application, we have to deal with unknown tokens\n",
    "# that were not present in the training corpus. However, for the sake\n",
    "# of clarity, we add all possible tokens from our dataset.\n",
    "# vocab.add(\"<unk>\")\n",
    "\n",
    "# Prepare vocabulary\n",
    "for s in dataframe_all.tokenized:\n",
    "    vocab.update(s)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(sorted(vocab))}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(sorted(vocab))}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch dataset for our thesis classification task\n",
    "\n",
    "class ThesisClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, word2idx):\n",
    "        self.data, self.labels = [], []\n",
    "        for tokens, label in zip(dataset, labels):\n",
    "            # Create inputs; map tokens to ids\n",
    "            self.data.append(torch.stack([\n",
    "                torch.tensor(word2idx[w], dtype=torch.long) for w in tokens\n",
    "            ]))\n",
    "\n",
    "            # Create labels; already an integer\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns one input and label sample\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the dot product attention mechanism and integrate it into your RNN classification model.\n",
    "\n",
    "2.2 Train and evaluate your models with a train-test-split (or optional 5-fold cross-validation).\n",
    "\n",
    "2.3 Assemble a table: Recall/Precision/F1 measure for RNN classification with and without attention. Do your results improve w.r.t. your old model?\n",
    "\n",
    "2.4 Can you find certain words that receive high attention weights regarding the decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement RNN classifier (nn.Module)\n",
    "### Notice: Think about padding for batch sizes > 1\n",
    "### Notice: 'torch.nn.utils.rnn' provides functionality\n",
    "### Notice: Here you can integrate the attention mechanism\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "class GRU_Classifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, num_classes,\n",
    "                 with_attention=False):\n",
    "        super(GRU_Classifier, self).__init__()\n",
    "        self.with_attention = with_attention\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_embeddings, \n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=False,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, X, lengths, hidden=None):\n",
    "        embeddings = self.embedding(X)\n",
    "\n",
    "        # Packed squence helps avoid unneccsary computation\n",
    "        packed_seq = pack_padded_sequence(embeddings, lengths)\n",
    "\n",
    "        outputs, hidden_states = self.rnn(packed_seq , hidden)\n",
    "\n",
    "        # If tuple (h_n, c_n) containts cell state c_n then select h_n\n",
    "        if isinstance(hidden_states, tuple):\n",
    "            hidden = hidden_states[0]\n",
    "        else:\n",
    "            hidden = hidden_states\n",
    "\n",
    "        clf_input, weights = hidden, None\n",
    "\n",
    "        # Apply classifier with hidden states\n",
    "        logits = self.fc(clf_input.squeeze(0))\n",
    "\n",
    "        return logits, hidden_states, weights\n",
    "\n",
    "\n",
    "class SequencePadder():\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        labels = [x[1] for x in sorted_batch]\n",
    "        padded = pad_sequence(sequences, padding_value=self.symbol)\n",
    "        lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        return padded, torch.LongTensor(labels), lengths\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the evaluation functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def eval(model, dataloader, criterion, device, return_attn_dict=False):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Initialize and train the RNN Classification Model for X epochs + Evaluation\n",
    "\n",
    "# Training parameters\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "DEVICE = \"cpu\" # 'cpu', 'mps' or 'cuda'\n",
    "LABEL_COL = \"label\"\n",
    "PAD_IDX = word2idx[\"<pad>\"]\n",
    "\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "### Notice: Data loading example\n",
    "data_samples = dataframe.tokenized\n",
    "data_labels = dataframe[LABEL_COL].values\n",
    "dataset = ThesisClassificationDataset(data_samples, data_labels, word2idx=word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=SequencePadder(PAD_IDX))\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f719873",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.4 Visualize the attention weights\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
