{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 6: Attention (please!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) Sentiment Analysis\n",
    "\n",
    "In this task, we'll use the kaggle Rotten Tomatoes Dataset for this exercise: [Source and Download instructions](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data).\n",
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset.\n",
    "The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order.\n",
    "Each sentence has been parsed into many phrases (chunks) using the Stanford parser.\n",
    "Each phrase has a `PhraseId`, each sentence a `SentenceId`.\n",
    "Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "### Data\n",
    "\n",
    "Rotten Tomatoes Dataset: `train.tsv` contains the phrases and their associated sentiment labels.\n",
    "We have additionally provided a `SentenceId` so that you can track which phrases belong to a single sentence.\n",
    "`test.tsv` contains just phrases; use your model to assign a sentiment label to each phrase.\n",
    "\n",
    "The sentiment labels are:\n",
    "\n",
    "* 0 - negative\n",
    "* 1 - somewhat negative\n",
    "* 2 - neutral\n",
    "* 3 - somewhat positive\n",
    "* 4 - positive\n",
    "\n",
    "### GloVe Word Embeddings\n",
    "\n",
    "Use GloVe word embeddings for your `nn.Embedding` layer, there is a number of pretrained models for English available in the `torchtext` module.\n",
    "You are free to  use any kind of attention and architecture you like.\n",
    "Just remember that the basic form for attention based networks is always and encoder / Decoder architecture.\n",
    "Use `torchtext.vocab.GloVe` to get started quickly with the word embeddings.\n",
    "\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sklearn_metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 As always: conduct some data preprocessing.\n",
    "\n",
    "1.2 Download and prepare the GloVe word embeddings. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Create a PyTorch Dataset class which handles your tokenized data with respect to input and (class) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_dataset(filepath):\n",
    "    \"\"\"Loads all phrase instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return pd.read_csv(filepath, header=0, sep='\\t')\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    def _preprocss_fn(text):\n",
    "        remove_pun = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        remove_digits = str.maketrans(string.digits, ' '*len(string.digits))\n",
    "        text = text.translate(remove_digits)\n",
    "        text = text.translate(remove_pun)\n",
    "        text = re.sub(' {2,}', ' ', text)\n",
    "        return text.lower()\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Remove punctuation, digits and lowercase phrases\n",
    "    dataframe[\"Phrase\"] = dataframe[\"Phrase\"].apply(lambda s: _preprocss_fn(s))\n",
    "\n",
    "    # Filter out empty phrases\n",
    "    dataframe = dataframe[dataframe[\"Phrase\"].str.len() > 1]\n",
    "\n",
    "    # Reset index of dataframe\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Simple tokenization of phrases\n",
    "    dataframe[\"tokenized\"] = [title.split() for title in dataframe[\"Phrase\"].values]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "train_dataframe = load_sentiment_dataset(\"data/rotten_tomatoes_train.tsv\")\n",
    "train_dataframe = preprocess(train_dataframe)\n",
    "\n",
    "# Map for formatting labels\n",
    "IDX2SENTIMENT = {0: \"negative\", 1: \"somewhat negative\", 2: \"neutral\",\n",
    "                 3: \"somewhat positive\", 4: \"positive\"}\n",
    "\n",
    "# Test labels not available and submission to Kaggle required\n",
    "# test_dataframe = load_sentiment_dataset(\"data/rotten_tomatoes_test.tsv\")\n",
    "# test_dataframe = preprocess(test_dataframe)\n",
    "\n",
    "print(f\"Num train dataset: {len(train_dataframe)}\")\n",
    "# print(f\"Num test dataset: {len(test_dataframe)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the pre-trained (english) GloVe embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Prepare glove embeddings\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "\n",
    "def append_special(glove, special, vec=None):\n",
    "    glove.itos.append(special)\n",
    "    glove.stoi[special] = glove.itos.index(special)\n",
    "    if vec is None:\n",
    "        vec = torch.zeros(1, glove.vectors.size(1))\n",
    "    glove.vectors = torch.cat((glove.vectors, vec))\n",
    "    return glove\n",
    "\n",
    "glove = GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "# We need to add some special tokens\n",
    "glove = append_special(glove, UNK_TOKEN)\n",
    "glove = append_special(glove, PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RottenTomatoesDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, glove, unk=\"<unk>\"):\n",
    "        self.data, self.labels = [], []\n",
    "        for tokens, label in zip(dataset, labels):\n",
    "            # Create inputs; map tokens to ids\n",
    "            self.data.append(torch.stack([\n",
    "                torch.tensor(glove.stoi.get(w, glove.stoi.get(unk)), dtype=torch.long) for w in tokens\n",
    "            ]))\n",
    "\n",
    "            # Create labels; already an integer\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns one input and label sample\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement and reuse your RNN-based classifciation models for the sentiment classification task. \n",
    "\n",
    "2.2 Train and evaluate your models by performing a train-test-split on the `train.tsv` file.\n",
    "\n",
    "2.3 Check and compare your classification results with some publicly available baselines (there are plenty of on the internet).\n",
    "\n",
    "2.4 Visualize the attention weights for the words and pick some nice samples for each sentiment category!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement RNN classifier (nn.Module)\n",
    "### Notice: Think about padding for batch sizes > 1\n",
    "### Notice: 'torch.nn.utils.rnn' provides functionality\n",
    "### Notice: Here you can integrate the attention mechanism\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "class GRU_Classifier(nn.Module):\n",
    "    def __init__(self, glove, hidden_dim, num_classes, with_attention=False):\n",
    "        super(GRU_Classifier, self).__init__()\n",
    "        self.with_attention = with_attention\n",
    "\n",
    "        # TODO: add glove embeddings\n",
    "        self.embedding = None\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=glove.dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=False,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, X, lengths, hidden=None):\n",
    "        embeddings = self.embedding(X)\n",
    "\n",
    "        # Packed squence helps avoid unneccsary computation\n",
    "        padded_seq = pack_padded_sequence(embeddings, lengths)\n",
    "\n",
    "        outputs, hidden_states = self.rnn(padded_seq, hidden)\n",
    "\n",
    "        # If tuple (h_n, c_n) containts cell state c_n then select h_n\n",
    "        if isinstance(hidden_states, tuple):\n",
    "            hidden = hidden_states[0]\n",
    "        else:\n",
    "            hidden = hidden_states\n",
    "\n",
    "        clf_input, weights = hidden, None\n",
    "\n",
    "        # Apply classifier with hidden states\n",
    "        logits = self.fc(clf_input.squeeze(0))\n",
    "\n",
    "        return logits, hidden_states, weights\n",
    "\n",
    "\n",
    "class SequencePadder():\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        labels = [x[1] for x in sorted_batch]\n",
    "        padded = pad_sequence(sequences, padding_value=self.symbol)\n",
    "        lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        return padded, torch.LongTensor(labels), lengths\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the evaluation functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def eval(model, dataloader, criterion, device, return_attn_dict=False):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Initialize and train the RNN Classification Model for X epochs + Evaluation\n",
    "\n",
    "# Training parameters\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "DEVICE = \"cpu\" # 'cpu', 'mps' or 'cuda'\n",
    "LABEL_COL = \"Sentiment\"\n",
    "PAD_IDX = glove.stoi[PAD_TOKEN]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "### Notice: Data loading example\n",
    "data_samples = train_dataframe.tokenized\n",
    "data_labels = train_dataframe[LABEL_COL].values\n",
    "dataset = RottenTomatoesDataset(data_samples, data_labels, glove=glove, unk=UNK_TOKEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=SequencePadder(PAD_IDX))\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.4 Visualize the attention weights\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
